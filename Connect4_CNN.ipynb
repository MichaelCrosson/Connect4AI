{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset from connect4_dataset_parallel.pkl with 443661 entries.\n",
      "Loaded dataset from connect4_rand.pkl with 141164 entries.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "save_path = \"connect4_dataset_parallel.pkl\"\n",
    "with open(save_path, \"rb\") as f:\n",
    "    dataset_original = pickle.load(f)\n",
    "\n",
    "save_path_skill = \"connect4_rand.pkl\"\n",
    "with open(save_path_skill, \"rb\") as f:\n",
    "    dataset_skill = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded dataset from {save_path} with {len(dataset_original)} entries.\")\n",
    "print(f\"Loaded dataset from {save_path_skill} with {len(dataset_skill)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 443661\n",
      "Filtered dataset size: 334489\n"
     ]
    }
   ],
   "source": [
    "# Group entries by `state` and determine the most common `best_move`\n",
    "def filter_duplicates(dataset):\n",
    "    state_to_moves = {}\n",
    "    \n",
    "    # Group all best_moves for each state\n",
    "    for entry in dataset:\n",
    "        state = entry['state']\n",
    "        best_move = entry['best_move']\n",
    "        if state not in state_to_moves:\n",
    "            state_to_moves[state] = []\n",
    "        state_to_moves[state].append(best_move)\n",
    "    \n",
    "    # Create a new dataset with the most common best_move for each state\n",
    "    filtered_dataset = []\n",
    "    for state, moves in state_to_moves.items():\n",
    "        most_common_move = Counter(moves).most_common(1)[0][0]  # Get the most common best_move\n",
    "        filtered_dataset.append({'state': state, 'best_move': most_common_move})\n",
    "    \n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "# weight the dataset\n",
    "# duplicate entries with an ideal move in column 2 or 4\n",
    "# duplicate TWICE entries with an ideal move in column 3\n",
    "# the middle column should be the preferred column, and its neighbors preferred next\n",
    "dataset_weighted = dataset_original.copy()\n",
    "for entry in dataset_original:\n",
    "    # print(entry)\n",
    "    ideal_move = entry['best_move']\n",
    "    if ideal_move == 2 or ideal_move == 4:\n",
    "        dataset_weighted.append(entry)\n",
    "    elif ideal_move == 3:\n",
    "        dataset_weighted.append(entry)\n",
    "        dataset_weighted.append(entry)\n",
    "\n",
    "filtered_dataset = filter_duplicates(dataset_weighted)\n",
    "\n",
    "print(f\"Original dataset size: {len(dataset_original)}\")\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 711921\n",
      "Filtered dataset size: 443661\n",
      "Filtered dataset size: 334489\n",
      "usable dataset size: 334489\n",
      "skilled dataset size: 141164\n"
     ]
    }
   ],
   "source": [
    "dataset = filtered_dataset.copy()\n",
    "print(f\"Filtered dataset size: {len(dataset_weighted)}\")\n",
    "print(f\"Filtered dataset size: {len(dataset_original)}\")\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n",
    "print(f\"usable dataset size: {len(dataset)}\")\n",
    "print(f\"skilled dataset size: {len(dataset_skill)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitboard_to_matrix(bitboard):\n",
    "    \"\"\"\n",
    "    Converts a bitboard integer into a 6x7 binary matrix.\n",
    "    \"\"\"\n",
    "    matrix = np.zeros((6, 7), dtype=np.int32)\n",
    "    for row in range(6):\n",
    "        for col in range(7):\n",
    "            position = row * 7 + col\n",
    "            if bitboard & (1 << position):\n",
    "                matrix[row, col] = 1\n",
    "    return matrix\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Converts the dataset into input matrices (X) and target labels (y).\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for entry in dataset:\n",
    "        player1_matrix = bitboard_to_matrix(entry['state'][0])\n",
    "        player2_matrix = bitboard_to_matrix(entry['state'][1])\n",
    "        # Combine both matrices into a 2-channel representation\n",
    "        combined_matrix = np.stack([player1_matrix, player2_matrix], axis=0)\n",
    "        X.append(combined_matrix)\n",
    "        y.append(entry['best_move'])  # The target is the best move\n",
    "    return np.array(X), np.array(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = preprocess_dataset(dataset_weighted)\n",
    "X_train_skill, y_train_skill = preprocess_dataset(dataset_skill)\n",
    "X_test, y_test = preprocess_dataset(filtered_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (X): (711921, 2, 6, 7)\n",
      "Target shape (y): (711921,)\n",
      "Input shape (X): (334489, 2, 6, 7)\n",
      "Target shape (y): (334489,)\n",
      "Input shape (X): (141164, 2, 6, 7)\n",
      "Target shape (y): (141164,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input shape (X):\", X_train.shape)  # Expected: (num_samples, 2, 6, 7)\n",
    "print(\"Target shape (y):\", y_train.shape)  # Expected: (num_samples,)\n",
    "print(\"Input shape (X):\", X_test.shape)  # Expected: (num_samples, 2, 6, 7)\n",
    "print(\"Target shape (y):\", y_test.shape)  # Expected: (num_samples,)\n",
    "print(\"Input shape (X):\", X_train_skill.shape)  # Expected: (num_samples, 2, 6, 7)\n",
    "print(\"Target shape (y):\", y_train_skill.shape)  # Expected: (num_samples,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class Connect4CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Connect4CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=7, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=7, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 6 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 7)  # 7 outputs for the 7 columns (best move)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten for the fully connected layer\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # No activation here; handled by loss function\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a validation or test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in dataloader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large, weighted dataset only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 1.6058\n",
      "Epoch 2: Training Loss = 1.5003\n",
      "Epoch 3: Training Loss = 1.4461\n",
      "Epoch 4: Training Loss = 1.4048\n",
      "Epoch 5: Training Loss = 1.3708\n",
      "Epoch 6: Training Loss = 1.3408\n",
      "Epoch 7: Training Loss = 1.3162\n",
      "Epoch 8: Training Loss = 1.2941\n",
      "Epoch 9: Training Loss = 1.2745\n",
      "Epoch 10: Training Loss = 1.2575\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for the large dataset\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                torch.tensor(y_train, dtype=torch.long))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = Connect4CNN()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}: Training Loss = {running_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.2451, Test Accuracy: 0.5241\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for the smaller dataset\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                torch.tensor(y_test, dtype=torch.long))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skilled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 1.7778\n",
      "Epoch 2: Training Loss = 1.6895\n",
      "Epoch 3: Training Loss = 1.6443\n",
      "Epoch 4: Training Loss = 1.6035\n",
      "Epoch 5: Training Loss = 1.5630\n",
      "Epoch 6: Training Loss = 1.5214\n",
      "Epoch 7: Training Loss = 1.4786\n",
      "Epoch 8: Training Loss = 1.4382\n",
      "Epoch 9: Training Loss = 1.3988\n",
      "Epoch 10: Training Loss = 1.3632\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for the skilled dataset\n",
    "train_dataset_skill = TensorDataset(torch.tensor(X_train_skill, dtype=torch.float32),\n",
    "                                torch.tensor(y_train_skill, dtype=torch.long))\n",
    "train_loader_skill = DataLoader(train_dataset_skill, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize and train the model\n",
    "model_skill = Connect4CNN()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_skill.parameters(), lr=0.001)\n",
    "\n",
    "# Train\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model_skill.train()\n",
    "    running_loss = 0\n",
    "    for batch_X, batch_y in train_loader_skill:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_skill(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}: Training Loss = {running_loss / len(train_loader_skill):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8927, Test Accuracy: 0.3114\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for the smaller dataset\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                torch.tensor(y_test, dtype=torch.long))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = evaluate_model(model_skill, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train on large dataset, validate on skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 1.6048, Validation Loss = 1.7614, Validation Accuracy = 0.2909\n",
      "Epoch 2: Training Loss = 1.5037, Validation Loss = 1.7242, Validation Accuracy = 0.3085\n",
      "Epoch 3: Training Loss = 1.4484, Validation Loss = 1.7335, Validation Accuracy = 0.3143\n",
      "Epoch 4: Training Loss = 1.4059, Validation Loss = 1.7431, Validation Accuracy = 0.3187\n",
      "Epoch 5: Training Loss = 1.3718, Validation Loss = 1.7727, Validation Accuracy = 0.3154\n",
      "Epoch 6: Training Loss = 1.3428, Validation Loss = 1.7964, Validation Accuracy = 0.3191\n",
      "Epoch 7: Training Loss = 1.3183, Validation Loss = 1.8133, Validation Accuracy = 0.3165\n",
      "Epoch 8: Training Loss = 1.2957, Validation Loss = 1.8737, Validation Accuracy = 0.3161\n",
      "Epoch 9: Training Loss = 1.2780, Validation Loss = 1.8542, Validation Accuracy = 0.3187\n",
      "Epoch 10: Training Loss = 1.2610, Validation Loss = 1.8908, Validation Accuracy = 0.3146\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                torch.tensor(y_train, dtype=torch.long))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "val_dataset_skill = TensorDataset(torch.tensor(X_train_skill, dtype=torch.float32),\n",
    "                                torch.tensor(y_train_skill, dtype=torch.long))\n",
    "val_loader_skill = DataLoader(val_dataset_skill, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize and train the model\n",
    "model_ls = Connect4CNN()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_ls.parameters(), lr=0.001)\n",
    "\n",
    "# Train\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')  # Track the best validation loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_ls.train()\n",
    "    running_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_ls(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    # Validate on the intermediate dataset\n",
    "    val_loss, val_accuracy = evaluate_model(model_ls, val_loader_skill, criterion)\n",
    "    print(f\"Epoch {epoch + 1}: Training Loss = {running_loss / len(train_loader):.4f}, \"\n",
    "            f\"Validation Loss = {val_loss:.4f}, Validation Accuracy = {val_accuracy:.4f}\")\n",
    "    # Save the model if validation loss improves\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model_ls.state_dict(), \"best_connect4_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desti\\AppData\\Local\\Temp\\ipykernel_21024\\2472268306.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_ls.load_state_dict(torch.load(\"best_connect4_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Loss: 1.5376, Test Accuracy: 0.3863\n"
     ]
    }
   ],
   "source": [
    "# Prepare the test loader\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                torch.tensor(y_test, dtype=torch.long))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the best model and test it\n",
    "model_ls.load_state_dict(torch.load(\"best_connect4_model.pth\"))\n",
    "test_loss, test_accuracy = evaluate_model(model_ls, test_loader, criterion)\n",
    "print(f\"Final Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
